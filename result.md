# 本地LLM模型测试评测报告

## 一、测试概述

本次测试对比评估了主流的本地部署大语言模型，涵盖了不同参数规模和量化版本：

### 1.1 测试模型清单

1. Gemma2系列
    - gemma2-9b-instruct-q8_0
    - gemma2-27b-instruct-q5_K_M

2. Qwen2.5系列
    - qwen2.5-7b-instruct-q8_0
    - qwen2.5-14b-instruct-q8_0
    - qwen2.5-14b-instruct-q5_K_M
    - qwen2.5-32b-instruct-q4_K_M
    - qwen2.5-32b-instruct-q5_K_M

3. Llama3.1系列
    - llama3.1-8b-instruct-q8_0

4. Phi3系列
    - phi3-14b-medium-4k-instruct-q8_0

5. Rombos系列
    - Rombos-LLM-V2.6-Qwen-14b-Q8_0

### 1.2 测试条件
- 并发请求数: 4
- 平均输入长度: 230 tokens (标准差: 100)
- 平均输出长度: 280 tokens (标准差: 100)
- 测试样本数: 每个模型16个请求

## 二、性能指标详细对比

### 2.1 首个令牌生成时间 (TTFT, 单位: 秒)

| 模型 | 平均值 | 中位数 | 最快响应 | P95 | 标准差 |
|------|--------|--------|----------|-----|--------|
| Llama3.1 8B | 2.43 | 1.99 | 0.34 | 5.91 | 1.91 |
| Qwen2.5 7B | 2.21 | 1.81 | 0.39 | 5.36 | 1.75 |
| Gemma2 9B | 3.44 | 2.78 | 0.44 | 8.25 | 2.69 |
| Qwen2.5 14B (Q8) | 4.40 | 3.69 | 0.62 | 10.23 | 3.25 |
| Qwen2.5 14B (Q5) | 3.80 | 4.49 | 0.76 | 5.51 | 1.63 |
| Phi3 14B | 4.95 | 4.42 | 0.93 | 10.82 | 3.32 |
| Rombos 14B | 3.09 | 3.80 | 0.77 | 4.98 | 1.62 |
| Gemma2 27B | 9.36 | 9.35 | 1.11 | 18.80 | 5.58 |
| Qwen2.5 32B (Q4) | 9.91 | 10.39 | 1.21 | 19.12 | 5.57 |
| Qwen2.5 32B (Q5) | 9.94 | 9.89 | 1.25 | 21.22 | 5.67 |

### 2.2 令牌间延迟 (单位: 秒)

| 模型 | 平均值 | 中位数 | 标准差 | P95 |
|------|--------|--------|--------|-----|
| Llama3.1 8B | 0.066 | 0.064 | 0.013 | 0.092 |
| Qwen2.5 7B | 0.062 | 0.064 | 0.011 | 0.080 |
| Gemma2 9B | 0.095 | 0.089 | 0.021 | 0.134 |
| Qwen2.5 14B (Q8) | 0.105 | 0.106 | 0.019 | 0.134 |
| Qwen2.5 14B (Q5) | 0.186 | 0.185 | 0.025 | 0.221 |
| Phi3 14B | 0.139 | 0.131 | 0.044 | 0.219 |
| Rombos 14B | 0.103 | 0.104 | 0.012 | 0.119 |
| Gemma2 27B | 0.409 | 0.419 | 0.058 | 0.480 |
| Qwen2.5 32B (Q4) | 0.312 | 0.318 | 0.047 | 0.365 |
| Qwen2.5 32B (Q5) | 0.437 | 0.467 | 0.068 | 0.523 |

### 2.3 端到端延迟 (单位: 秒)

| 模型 | 平均值 | 中位数 | P95 | 标准差 |
|------|--------|--------|-----|--------|
| Llama3.1 8B | 10.49 | 10.48 | 15.46 | 3.16 |
| Qwen2.5 7B | 10.67 | 10.20 | 16.04 | 3.39 |
| Gemma2 9B | 11.77 | 12.06 | 16.54 | 3.60 |
| Qwen2.5 14B (Q8) | 24.83 | 24.20 | 36.35 | 6.78 |
| Qwen2.5 14B (Q5) | 46.95 | 44.88 | 62.66 | 11.34 |
| Phi3 14B | 23.95 | 21.02 | 38.88 | 7.56 |
| Rombos 14B | 28.06 | 26.81 | 41.98 | 8.84 |
| Gemma2 27B | 66.36 | 70.03 | 92.92 | 19.23 |
| Qwen2.5 32B (Q4) | 57.58 | 58.59 | 87.33 | 17.62 |
| Qwen2.5 32B (Q5) | 82.15 | 86.58 | 106.82 | 20.40 |

### 2.4 吞吐量表现

| 模型 | 平均吞吐量(tokens/s) | 请求吞吐量(req/min) | 平均输出长度(tokens) |
|------|---------------------|-------------------|-------------------|
| Llama3.1 8B | 50.77 | 19.09 | 159.56 |
| Qwen2.5 7B | 48.41 | 16.58 | 175.19 |
| Gemma2 9B | 35.34 | 16.99 | 124.81 |
| Qwen2.5 14B (Q8) | 31.67 | 7.82 | 242.88 |
| Qwen2.5 14B (Q5) | 19.47 | 4.51 | 259.00 |
| Phi3 14B | 23.20 | 7.28 | 191.13 |
| Rombos 14B | 32.60 | 7.05 | 277.63 |
| Gemma2 27B | 8.90 | 3.19 | 167.38 |
| Qwen2.5 32B (Q4) | 11.49 | 3.62 | 190.69 |
| Qwen2.5 32B (Q5) | 8.23 | 2.55 | 193.88 |

## 三、重要发现

### 3.1 参数规模影响

1. 响应速度
    - 7B-9B模型的TTFT普遍在2-3秒
    - 14B模型的TTFT在3-5秒
    - 27B-32B模型的TTFT显著增加到9-10秒
    - 模型大小与响应延迟呈明显正相关

2. 吞吐量
    - 小模型(7B-9B)吞吐量显著高于大模型
    - Llama3.1 8B和Qwen2.5 7B的吞吐量最高(>45 tokens/s)
    - 32B模型吞吐量显著下降(8-11 tokens/s)

### 3.2 量化效果分析

1. Qwen2.5 14B的量化对比
    - Q8版本比Q5版本性能提升约62%(31.67 vs 19.47 tokens/s)
    - Q8版本端到端延迟减少约47%(24.83 vs 46.95秒)
    - 量化精度对性能影响显著

2. Qwen2.5 32B的量化对比
    - Q4版本比Q5版本性能略好
    - 端到端延迟: Q4为57.58秒，Q5为82.15秒
    - 在大模型上，低精度量化可能更具优势

### 3.3 模型特点分析

1. Llama3.1 8B
    - 综合性能最优
    - 最低的端到端延迟
    - 最高的吞吐量
    - 响应稳定性好

2. Qwen2.5系列
    - 7B版本性能接近Llama3.1
    - 14B版本在性能和能力上较为均衡
    - 32B版本虽然延迟高但输出质量可期

3. Gemma2系列
    - 9B版本性能适中
    - 27B版本延迟显著，适合对效果要求高的场景

4. Phi3和Rombos
    - 处于14B模型的正常性能范围
    - Rombos在输出长度上较为突出

## 四、部署建议

### 4.1 场景选择

1. 高并发场景
    - 推荐: Llama3.1 8B, Qwen2.5 7B
    - 特点: 低延迟、高吞吐
    - 适用: 实时对话、简单问答

2. 均衡场景
    - 推荐: Qwen2.5 14B (Q8), Phi3 14B
    - 特点: 性能与效果平衡
    - 适用: 通用对话、内容生成

3. 效果优先场景
    - 推荐: Qwen2.5 32B, Gemma2 27B
    - 特点: 能力强、延迟较高
    - 适用: 复杂推理、专业分析

### 4.2 资源配置建议

1. 显存需求
    - 7B-9B模型: 最低16GB
    - 14B模型: 建议24GB以上
    - 27B-32B模型: 建议40GB以上

2. 量化策略
    - 资源受限时可考虑低精度量化
    - 14B及以下模型优先考虑Q8
    - 大模型可考虑Q4/Q5以降低资源占用

3. 并发处理
    - 小模型可支持更高并发
    - 大模型建议限制并发数
    - 根据实际资源调整批处理大小

## 五、总结建议

1. 模型选择
    - 明确场景需求和资源约束
    - 小规模场景优先考虑7B-9B模型
    - 大规模场景需要在性能和效果间权衡

2. 优化方向
    - 合理使用量化降低资源占用
    - 优化推理引擎和批处理策略
    - 建立性能监控和告警机制

3. 部署实践
    - 进行充分的性能测试
    - 预留足够的资源余量
    - 建立完善的监控体系
    - 制定清晰的扩容策略